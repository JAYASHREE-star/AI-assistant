# ğŸ§  Unity AI Virtual Assistant

An intelligent, voice-interactive 3D assistant powered by Unity and AI technologies like OpenAI GPT, Azure Speech, and real-time APIs. This assistant can listen, understand, speak, animate, and perform desktop tasks â€” bridging immersive UX and smart automation.

> ğŸš€ Designed as a portfolio-grade project showcasing AI integration, speech interfaces, and Unity development in a practical real-world assistant app.

---

ğŸ§© Key Features

 1.ğŸ”Š Voice Interaction
- Microphone-based speech recognition (Windows or Azure)
- Wake word detection (e.g., "Hey Ava") via Picovoice or custom trigger

2.ğŸ§  Conversational AI
- Dynamic responses using OpenAI's GPT (ChatGPT-like behavior)
- Multi-turn conversation with short-term memory
- Emotion-aware replies using voice tone or context (via sentiment API)

3.ğŸ—£ï¸ Realistic Speaking Avatar
- 3D rigged assistant (Mixamo + Unity Animator)
- Auto lip sync (SALSA LipSync) and expressive facial animations
- Custom animations mapped to emotions or task responses

4.ğŸ“± Task Execution
- Open installed apps (calculator, browser, notepad)
- Tell weather, date, time
- Launch websites or play music via voice

5.ğŸŒ Real-Time API Integration
- Weather (OpenWeatherMap)
- News headlines (NewsAPI or Bing News Search)
- Optional: Currency converter, calendar sync, reminders

6.ğŸ“‹ UI/UX Features
- Scrollable chat panel (user + assistant messages)
- Dynamic UI feedback (listening, thinking, responding states)
- Dark/light mode toggle
- Voice-to-text transcript view
- Typing fallback when mic is off

---

  ğŸ§° Tech Stack

| Feature        | Stack/Tools                                 |
|----------------|----------------------------------------------|
| Game Engine    | Unity 2021+ (URP or HDRP optional)           |
| Language       | C#                                           |
| NLP            | OpenAI GPT-3.5 / GPT-4 API                   |
| Voice Input    | Azure Speech SDK / System.Speech             |
| TTS            | Azure Neural Voices / Windows TTS            |
| Wake Word      | Porcupine Wake Word SDK (Unity Plugin)       |
| Lip Sync       | SALSA LipSync + Unity Blend Shapes           |
| Avatar Rigging | Mixamo + Animator Controller                 |
| Weather API    | OpenWeatherMap                               |
| UI             | TextMeshPro, Unity UI Toolkit                |

---

## ğŸ¬ Demo Preview
 
> [ğŸ”— GitHub Repo](https://github.com/JAYASHREE-star/AI-assistant)

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/unity-ai-assistant.git
````

### 2. Open in Unity

Use Unity Hub (2021+ recommended). Ensure the microphone is enabled in Editor permissions.

### 3. Install Dependencies

* [TextMeshPro](https://docs.unity3d.com/Packages/com.unity.textmeshpro@3.0/manual/index.html)
* [Azure Speech SDK for Unity](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/quickstarts)
* [SALSA LipSync](https://assetstore.unity.com/packages/tools/audio/salsa-lipsync-suite-148442)
* [Picovoice Porcupine (optional)](https://github.com/Picovoice/porcupine)

### 4. Configure API Keys

Create a `config.cs` or use environment variables.

```csharp
public static class Keys {
    public const string OPENAI_KEY = "sk-...";
    public const string AZURE_TTS_KEY = "...";
    public const string WEATHER_API_KEY = "...";
}
```

---

## ğŸ—ƒï¸ Project Structure

```
Assets/
â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ AI/OpenAIHandler.cs
â”‚   â”œâ”€â”€ Voice/VoiceRecognizer.cs
â”‚   â”œâ”€â”€ TTS/TextToSpeech.cs
â”‚   â”œâ”€â”€ System/TaskExecutor.cs
â”‚   â”œâ”€â”€ Utilities/EmotionAnalyzer.cs
â”œâ”€â”€ UI/
â”‚   â”œâ”€â”€ ChatManager.cs
â”‚   â”œâ”€â”€ AudioStateIndicator.cs
â”œâ”€â”€ Prefabs/
â”‚   â”œâ”€â”€ AssistantAvatar.prefab
â”œâ”€â”€ Resources/
â”‚   â””â”€â”€ Sounds/
â””â”€â”€ StreamingAssets/
    â””â”€â”€ Config/
        â””â”€â”€ keys.json
```

---

## 1.ğŸŒŸ Advanced Ideas for Future Development

* ğŸ§  **Long-Term Memory**: Store user preferences, names, prior context using SQLite or JSON DB.
* ğŸŒ **Browser UI Integration**: WebView or WebGL build for Web-based assistant.
* ğŸ¤³ **AR Mode**: Integrate with Unity AR Foundation to place assistant in real-world environment.
* ğŸ§  **Self-Learning**: Fine-tune assistant behavior based on feedback or usage patterns.
* ğŸ“¸ **Facial Recognition**: Detect user via webcam and tailor conversation.

---


### 2. Open in Unity

Use Unity Hub (2021+ recommended). Ensure the microphone is enabled in Editor permissions.

### 3. Install Dependencies

* [TextMeshPro](https://docs.unity3d.com/Packages/com.unity.textmeshpro@3.0/manual/index.html)
* [Azure Speech SDK for Unity](https://learn.microsoft.com/en-us/azure/cognitive-services/speech-service/quickstarts)
* [SALSA LipSync](https://assetstore.unity.com/packages/tools/audio/salsa-lipsync-suite-148442)
* [Picovoice Porcupine (optional)](https://github.com/Picovoice/porcupine)

### 4. Configure API Keys

Create a `config.cs` or use environment variables.

```csharp
public static class Keys {
    public const string OPENAI_KEY = "sk-...";
    public const string AZURE_TTS_KEY = "...";
    public const string WEATHER_API_KEY = "...";
}
```

---

## ğŸ—ƒï¸ Project Structure

```
Assets/
â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ AI/OpenAIHandler.cs
â”‚   â”œâ”€â”€ Voice/VoiceRecognizer.cs
â”‚   â”œâ”€â”€ TTS/TextToSpeech.cs
â”‚   â”œâ”€â”€ System/TaskExecutor.cs
â”‚   â”œâ”€â”€ Utilities/EmotionAnalyzer.cs
â”œâ”€â”€ UI/
â”‚   â”œâ”€â”€ ChatManager.cs
â”‚   â”œâ”€â”€ AudioStateIndicator.cs
â”œâ”€â”€ Prefabs/
â”‚   â”œâ”€â”€ AssistantAvatar.prefab
â”œâ”€â”€ Resources/
â”‚   â””â”€â”€ Sounds/
â””â”€â”€ StreamingAssets/
    â””â”€â”€ Config/
        â””â”€â”€ keys.json
```

---

## ğŸŒŸ Advanced Ideas for Future Development

* ğŸ§  **Long-Term Memory**: Store user preferences, names, prior context using SQLite or JSON DB.
* ğŸŒ **Browser UI Integration**: WebView or WebGL build for Web-based assistant.
* ğŸ¤³ **AR Mode**: Integrate with Unity AR Foundation to place assistant in real-world environment.
* ğŸ§  **Self-Learning**: Fine-tune assistant behavior based on feedback or usage patterns.
* ğŸ“¸ **Facial Recognition**: Detect user via webcam and tailor conversation.

---

## ğŸ¤ Contributing

Want to contribute? Here's how:

```bash
1. Fork the repo
2. Create your feature branch (git checkout -b feature/YourFeature)
3. Commit your changes (git commit -m 'Add Feature')
4. Push to the branch (git push origin feature/YourFeature)
5. Open a Pull Request
```

---

## â­ If you like this project...

* Leave a ğŸŒŸ on GitHub
* Share the demo video on LinkedIn
